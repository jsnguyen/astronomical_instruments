import importlib.resources
import fnmatch

import numpy as np
from numpy.polynomial import Polynomial
from astropy.io import fits
import matplotlib.pyplot as plt

from astronomical_instruments.utils import *

class NIRC2:
    def apply_nonlinearity_correction(data, year, correction_filename=None):
        '''
        apply the non-linearity correction

        args:
            data: the image we want to linearize
            year: the year of data we want to use, 2019 for before the NIRC2 upgrade, 2024 for after the NIRC2 upgrade
            correction_filename: can specify a new correction curve file. must be a *.npy file generated by this script!

        return:
            the corrected, linearized data
        '''

        if correction_filename is None:

            valid_years = ['2019', '2024']
            if year not in valid_years:
                print(f'Year must be in {valid_years}!')
                return

            data_file = importlib.resources.files(f'astronomical_instruments.data_{year}').joinpath(f'correction_curve_{year}.npy')
            coef = np.load(data_file, allow_pickle=True)
        else:
            coef = np.load(correction_filename, allow_pickle=True)

        correction_curve = Polynomial(coef)

        # want to apply the negative of this data to "push" the points up to meet the expected linearity
        correction_function = 1-correction_curve 

        # get the "percent correction" ie: the percentage to multiply by to correct for linearity
        percent_correction = correction_function(data)

        # apply final correction to get final data
        corrected_data = percent_correction*data

        return corrected_data

    def make_bad_pixel_mask(self, dark_frames, flat_frames, n_sigma_darks=1.5, n_sigma_flats = 3, n_div=128, box_radius=7):
        '''
        this is a general function, but has only been tested on NIRC2 data
        makes a bad pixel mask from dark frames and flat frames
        the default values for the sigma clipping and median were chosen by eye, so mileage may vary
        does a standard sigma clip on the dark stack, but a binned sigma clip on the flats, and a neighbor comparison on the flats
        combining all three of these steps plus the line mask makes the final bad pixel mask
        the neighbor comparison should cover some of the edge cases from the flat median binning sigma clip.

        args:
            dark_frames: numpy arrays of the stack of darks. should be the same exposure/coadds.
            flat_frames: numpy arrays of the stack of flats. should be the same exposure/coadds.
            n_sigma_darks: number of standard deviations above the median for dark frames to clip
            n_sigma_flats: number of standard deviations above the median for flat frames to clip
            n_div: the number of divisions to subdivide the flat into when doing the binned median sigma clip
            box_radius: the extent of pixels to use when doing the pixel median neighbor comparison

        returns:
            bad_pixel_mask: the final bad pixel mask
        '''
        
        print('Sigma clipping darks...')
        dark_median_stack = np.median(dark_frames, axis=0)
        dark_sigma_clip_mask = median_sigma_clip(dark_median_stack, n_sigma_darks)

        print('Sigma clipping flat subsections...')
        flat_median_stack = np.median(flat_frames, axis=0)
        flat_sigma_clip_mask = section_median_sigma_clip(flat_median_stack, (n_div, n_div), n_sigma_flats)

        print('Median compare neighbors flat...')
        flat_median_compare_mask = median_compare_neighbors(flat_median_stack, box_radius, n_sigma_flats)

        # these are using the best params for the NIRC2 diagonal crack
        print('Diagonal line mask...')
        nirc2_line_mask = diagonal_line_mask(flat_sigma_clip_mask, 1.653, 8, 3, xlim=[0, 217])

        print('Combining...')
        bad_pixel_mask = np.zeros(dark_frames[0].shape)
        bad_pixel_mask = np.logical_or(bad_pixel_mask, dark_sigma_clip_mask)
        bad_pixel_mask = np.logical_or(bad_pixel_mask, flat_sigma_clip_mask)
        bad_pixel_mask = np.logical_or(bad_pixel_mask, flat_median_compare_mask)
        bad_pixel_mask = np.logical_or(bad_pixel_mask, nirc2_line_mask)

        print('Bad Pixel Count')
        print(f'darks sigma clip {np.count_nonzero(dark_sigma_clip_mask)}')
        print(f'flats sigma clip {np.count_nonzero(flat_sigma_clip_mask)}')
        print(f'flats median compare {np.count_nonzero(flat_median_compare_mask)}')
        print(f'line mask {np.count_nonzero(nirc2_line_mask)}')
        print(f'Total {np.count_nonzero(bad_pixel_mask)}')

        return bad_pixel_mask

    def make_bad_pixel_mask_20230101(self):

        data_folder = importlib.resources.files('nirc2_darks')
        dark_files = []
        for resource in data_folder.iterdir():
            if fnmatch.fnmatch(resource.name, '*.fits'):
                dark_files.append(resource)
        dark_files = sorted(dark_files)

        data_folder = importlib.resources.files('nirc2_flats')
        flat_files = []
        for resource in data_folder.iterdir():
            if fnmatch.fnmatch(resource.name, '*.fits'):
                flat_files.append(resource)
        flat_files = sorted(flat_files)

        dark_frames = []
        dark_headers = []
        for fn in dark_files:
            header = fits.getheader(fn)
            if header['NAXIS1'] == 1024 and header['ITIME']==0.181 and header['COADDS']==100:
                dark_frames.append(fits.getdata(fn))
                dark_headers.append(header)

        flat_frames = []
        flat_headers = []
        for fn in flat_files:
            header = fits.getheader(fn)
            if header['NAXIS1'] == 1024:
                flat_frames.append(fits.getdata(fn))
                flat_headers.append(header)

        bad_pixel_mask = self.make_bad_pixel_mask(dark_frames, flat_frames)

        save_name = 'bad_pixel_mask_20230101.fits'
        print(f'Saving to {save_name}')

        hdu = fits.PrimaryHDU(data=bad_pixel_mask.astype(np.uint8))
        hdul = fits.HDUList([hdu])
        hdul.writeto(save_name, overwrite=True)

    def plot_linearity(self, linearity_data, n_fit, year, use_correction_curve):

        fig, ax = plt.subplots(figsize=(6,6))
        for bandname in linearity_data.keys():
            exp_times, linearity_line, full_data_mean, full_data_std = linearity_data[bandname]

            ax.errorbar(exp_times, full_data_mean, yerr=full_data_std, marker='o', linestyle='none', label=f'Average {bandname.capitalize()}-Band', alpha=0.7)
            #ax.plot(fit_xs, np.vectorize(res)(fit_xs), linestyle='-', color=ax.get_lines()[-1].get_c(), label='Polynomial Fit')
            ax.plot(exp_times[:n_fit], linearity_line(exp_times[:n_fit]), linestyle='--', color='black', alpha=0.7)
            ax.plot(exp_times[n_fit-1:], linearity_line(exp_times[n_fit-1:]), linestyle='--', color='black', alpha=0.3)

            if use_correction_curve:
                ax.set_title(f'CHECK Linearity {year}')
            else:
                ax.set_title(f'Linearity {year}')

            ax.set_xlabel('Exposure Time [s]')
            ax.set_ylabel('Average Pixel Count [DN/px]')
            
            ax.set_xlim(0,125)
            ax.set_ylim(0,1.25*np.max(full_data_mean))
            
        ax.grid()
        ax.legend()   

        if use_correction_curve:
            plt.savefig(f'CHECK_linearity_{year}.png', bbox_inches='tight')
        else:
            plt.savefig(f'linearity_{year}.png', bbox_inches='tight')

    def plot_linearity_deviation(self, linearity_data, correction_curves, diffs, cutoff, year, use_correction_curve):
        
        means = []
        fig, ax = plt.subplots(figsize=(6,6))
        for bandname in correction_curves.keys():
            nonlinearity_fit = correction_curves[bandname]
            diff = diffs[bandname]

            _, _, full_data_mean, full_data_std = linearity_data[bandname]
            weights = full_data_std/full_data_mean # higher number = more weight
            means.append(full_data_mean)

            fit_xs = np.linspace(np.min(full_data_mean), np.max(full_data_mean), 1000)

            ax.errorbar(full_data_mean[cutoff:], diff[cutoff:]*100, yerr=np.abs(weights[cutoff:]*diff[cutoff:]*100), marker='o', alpha=0.7, label=f'Percent Deviation {bandname.capitalize()}-Band')
            ax.plot(fit_xs, np.vectorize(nonlinearity_fit)(fit_xs)*100, linestyle='-', color=ax.get_lines()[-1].get_c(), label='2nd Order Polynomial Fit')

            if use_correction_curve:
                ax.set_title(f'CHECK Deviation From Linearity {year}')
            else:
                ax.set_title(f'Deviation From Linearity {year}')

        ax.set_xlabel('Average Pixel Count [DN/px]')
        ax.set_ylabel('Percent Deviation From Linearity [%]')

        ax.set_xlim(0,1.25*np.max(means))
        ax.set_ylim(-20,10)

        ax.grid()
        ax.legend()

        if use_correction_curve:
            plt.savefig(f'CHECK_linearity_diff_{year}.png', bbox_inches='tight')
        else:
            plt.savefig(f'linearity_diff_{year}.png', bbox_inches='tight')

    def fit_linearity(self, data, headers, cutoff, n_fit, order=1):
        
        data_file = importlib.resources.files('astronomical_instruments.data').joinpath(f'nirc2mask.fits')
        bad_pixel_mask = fits.getdata(data_file)
        
        exp_times = np.array([h['ITIME'] for h in headers])[cutoff:]
        data = data[cutoff:]
        headers = headers[cutoff:]

        full_data_mean = []
        full_data_std = []
        for d in data:
            masked = np.ma.array(d, mask=bad_pixel_mask)
            full_data_mean.append(np.ma.mean(masked))
            full_data_std.append(np.ma.std(masked))

        full_data_mean = np.array(full_data_mean)
        full_data_std = np.array(full_data_std)
        
        weights = np.square(full_data_std/full_data_mean) # higher number = more weight

        # fitting a line to the first n points
        linearity_line = Polynomial.fit(exp_times[:n_fit], full_data_mean[:n_fit], order, w=weights[:n_fit])

        return exp_times, linearity_line, full_data_mean, full_data_std

    def calc_correction_curves(self, linearity_data, correction_curve_slice, order=1):

        dn, up = correction_curve_slice

        diffs = {}
        correction_curves = {}
        for bandname in linearity_data.keys():
            exp_times, linearity_line, full_data_mean, full_data_std = linearity_data[bandname]

            expected = linearity_line(exp_times)
            diff = (full_data_mean - expected) /  expected
            weights = full_data_std/full_data_mean # higher number = more weight

            nonlinearity_fit = Polynomial.fit(full_data_mean[dn:up], diff[dn:up], order, w=weights[dn:up])
            print(f'{bandname.capitalize()}-Band Fit parameters: {nonlinearity_fit.convert().coef}')

            correction_curves[bandname] = nonlinearity_fit
            diffs[bandname] = diff

        return correction_curves, diffs

    def linearity(self, year, cutoff=None, correction_curve_slice=None, use_correction_curve=False, save_curves=True, order=1):
        '''
        read in the datasets, fit the linearity, make plots, and save the final correction curves

        args:
            year: can be '2019' or '2024' only, the two data sets that are available
            cutoff: the lower end cutoff for the fits
            correction_curve_slice: slices the corrected diff data to chop off the beginning or end to do a fit on the linear part of the system
            use_correction_curve: uses the pre-computed correction curve to check if it worked correctly
            save_curves: save .npy files
            order: the order of the final fit for the correction curve

        return:
            None
        '''

        valid_years = ['2019', '2024']
        if year not in valid_years:
            print(f'Year must be in {valid_years}!')
            return

        # cutoffs are needed in the fit to fit a curve better to the linear region in the detector

        if year == '2024':
            bands = ['kp', 'ks'] # h band seems to be bad for this data
            cutoff = 1 # cutoff of the beginning few elements that are bad
            correction_curve_slice = 3,-1 # cutoff the first few, and the last one to get a better fit
            n_fit = 5 # fit to the first n elements

        elif year == '2019':
            bands = ['h']
            cutoff = 0 # cutoff of the beginning few elements that are bad
            correction_curve_slice = 3,-2 # cutoff the first few and last one to get a better fit
            n_fit = 6 # fit to the first n elements

        linearity_data = {}
        for bandname in bands:

            # read in data
            data_folder = importlib.resources.files(f'astronomical_instruments.data_{year}.linearity_{bandname}')
            data_files = []
            for resource in data_folder.iterdir():
                if fnmatch.fnmatch(resource.name, '*.fits'):
                    data_files.append(resource)
            data_files = sorted(data_files)

            data = []
            headers = []
            for fn in data_files:
                d = fits.getdata(fn)/4 # divide by 4 to get back into units of DN
                if use_correction_curve:
                    d = NIRC2.apply_nonlinearity_correction(d, year)

                data.append(d) 
                headers.append(fits.getheader(fn))


            # fit a line to each dataset, cutting off the first n "cutoff" values
            # there's some weird parts near the beginning at extremely short exposures that don't match up with linearity
            # so we toss those and fit to a range that is closer to what we would take in science frames

            res = self.fit_linearity(data, headers, cutoff, n_fit)
            linearity_data[bandname] = res

        # calculate the correction curves based off of the "percent deviation from linearity"
        # percent = perfect_linear - actual_data / perfect_linear
        correction_curves, diffs = self.calc_correction_curves(linearity_data, correction_curve_slice, order=order)

        if save_curves:
            filename = f'correction_curve_{year}.npy'
            if len(correction_curves.keys()) > 1:
                for key in correction_curves.keys():
                    correction_curves[key] = correction_curves[key].convert(domain=[0, 10000])
                np.save(filename, np.mean(list(correction_curves.values())).convert().coef)
            else:
                np.save(filename, list(correction_curves.values())[0].convert().coef)

        self.plot_linearity(linearity_data, n_fit, year, use_correction_curve)
        self.plot_linearity_deviation(linearity_data, correction_curves, diffs, cutoff, year, use_correction_curve)